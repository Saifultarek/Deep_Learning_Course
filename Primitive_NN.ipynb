{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym-WirBSLmH9"
      },
      "source": [
        "## Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMlhUn-tLmID",
        "outputId": "567de85a-0f65-40f3-e9a8-e953c4a329d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 30, 30, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 13, 13, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 4, 4, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                65600     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 122,570\n",
            "Trainable params: 122,570\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define the CNN model\n",
        "def build_custom_cnn(input_shape=(32, 32, 3), num_classes=10):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Convolutional layer 1\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Convolutional layer 2\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Convolutional layer 3\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "    # Flatten the tensor output\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Dense layer\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example usage:\n",
        "model = build_custom_cnn(input_shape=(32, 32, 3), num_classes=10)\n",
        "model.summary()\n",
        "\n",
        "# Compilation\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# (Optional) Training code example\n",
        "# Assuming you have training_data and training_labels:\n",
        "# model.fit(training_data, training_labels, epochs=10, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sdkm3sT5LmIG"
      },
      "source": [
        "### Recurrent Neural Network (RNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZ9sRtapLmIH",
        "outputId": "87ab1c43-e3d1-4b92-eacd-1a716e5d3c9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn (SimpleRNN)       (None, 10, 50)            2600      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10, 1)             51        \n",
            "=================================================================\n",
            "Total params: 2,651\n",
            "Trainable params: 2,651\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "25/25 [==============================] - 3s 15ms/step - loss: 1.0413 - val_loss: 1.0070\n",
            "Epoch 2/10\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 1.0225 - val_loss: 1.0072\n",
            "Epoch 3/10\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 1.0196 - val_loss: 1.0044\n",
            "Epoch 4/10\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 1.0204 - val_loss: 1.0130\n",
            "Epoch 5/10\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 1.0160 - val_loss: 1.0046\n",
            "Epoch 6/10\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 1.0151 - val_loss: 0.9976\n",
            "Epoch 7/10\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 1.0136 - val_loss: 1.0031\n",
            "Epoch 8/10\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 1.0168 - val_loss: 1.0037\n",
            "Epoch 9/10\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 1.0170 - val_loss: 0.9966\n",
            "Epoch 10/10\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 1.0134 - val_loss: 1.0006\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "\n",
        "# Define constants\n",
        "input_sequence_length = 10  # Example sequence length, modify as needed\n",
        "input_features = 1          # Number of input features\n",
        "output_features = 1         # Number of output features\n",
        "hidden_units = 50           # Number of hidden units in the RNN layer\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(hidden_units, activation='tanh', input_shape=(input_sequence_length, input_features), return_sequences=True))\n",
        "model.add(Dense(output_features, activation='linear'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "# For this, you'll need training data. Here's an example of dummy data:\n",
        "X_train = np.random.randn(1000, input_sequence_length, input_features)\n",
        "Y_train = np.random.randn(1000, input_sequence_length, output_features)\n",
        "model.fit(X_train, Y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "#Predict with the model\n",
        "prediction = model.predict(np.random.randn(1, input_sequence_length, input_features))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUkTrLwmLmII"
      },
      "source": [
        "## Radial Basis Function Network (RBFN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czI0OH7ELmII",
        "outputId": "47ffb806-423f-4021-decc-5629b765cd97"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\deepn\\anaconda3\\envs\\data\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "c:\\Users\\deepn\\anaconda3\\envs\\data\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 1.2158515\n",
            "Loss: 0.6635001\n",
            "Loss: 0.40345496\n",
            "Loss: 0.28007412\n",
            "Loss: 0.22061619\n",
            "Loss: 0.19108361\n",
            "Loss: 0.17558569\n",
            "Loss: 0.1666943\n",
            "Loss: 0.16093613\n",
            "Loss: 0.15668632\n",
            "Loss: 0.15318385\n",
            "Loss: 0.1500718\n",
            "Loss: 0.14718172\n",
            "Loss: 0.14443354\n",
            "Loss: 0.14178866\n",
            "Loss: 0.13922764\n",
            "Loss: 0.13674024\n",
            "Loss: 0.13432051\n",
            "Loss: 0.13196453\n",
            "Loss: 0.12966943\n",
            "Loss: 0.12743284\n",
            "Loss: 0.12525263\n",
            "Loss: 0.12312684\n",
            "Loss: 0.12105373\n",
            "Loss: 0.11903152\n",
            "Loss: 0.11705858\n",
            "Loss: 0.11513332\n",
            "Loss: 0.11325424\n",
            "Loss: 0.11141986\n",
            "Loss: 0.10962878\n",
            "Loss: 0.107879676\n",
            "Loss: 0.10617123\n",
            "Loss: 0.10450219\n",
            "Loss: 0.10287139\n",
            "Loss: 0.1012776\n",
            "Loss: 0.09971979\n",
            "Loss: 0.09819686\n",
            "Loss: 0.09670775\n",
            "Loss: 0.09525149\n",
            "Loss: 0.09382711\n",
            "Loss: 0.09243371\n",
            "Loss: 0.091070384\n",
            "Loss: 0.08973629\n",
            "Loss: 0.08843057\n",
            "Loss: 0.08715245\n",
            "Loss: 0.08590115\n",
            "Loss: 0.084675916\n",
            "Loss: 0.08347606\n",
            "Loss: 0.082300894\n",
            "Loss: 0.08114971\n",
            "Loss: 0.08002192\n",
            "Loss: 0.078916825\n",
            "Loss: 0.0778339\n",
            "Loss: 0.076772526\n",
            "Loss: 0.075732134\n",
            "Loss: 0.0747122\n",
            "Loss: 0.0737122\n",
            "Loss: 0.07273161\n",
            "Loss: 0.071769945\n",
            "Loss: 0.07082675\n",
            "Loss: 0.069901526\n",
            "Loss: 0.06899385\n",
            "Loss: 0.06810331\n",
            "Loss: 0.067229465\n",
            "Loss: 0.06637189\n",
            "Loss: 0.06553024\n",
            "Loss: 0.064704105\n",
            "Loss: 0.06389313\n",
            "Loss: 0.06309697\n",
            "Loss: 0.062315248\n",
            "Loss: 0.06154765\n",
            "Loss: 0.06079384\n",
            "Loss: 0.060053512\n",
            "Loss: 0.059326325\n",
            "Loss: 0.058612052\n",
            "Loss: 0.05791034\n",
            "Loss: 0.05722096\n",
            "Loss: 0.05654359\n",
            "Loss: 0.055878006\n",
            "Loss: 0.05522392\n",
            "Loss: 0.054581095\n",
            "Loss: 0.05394929\n",
            "Loss: 0.053328276\n",
            "Loss: 0.05271781\n",
            "Loss: 0.05211766\n",
            "Loss: 0.05152765\n",
            "Loss: 0.050947517\n",
            "Loss: 0.050377086\n",
            "Loss: 0.049816165\n",
            "Loss: 0.04926452\n",
            "Loss: 0.048722006\n",
            "Loss: 0.048188403\n",
            "Loss: 0.047663547\n",
            "Loss: 0.047147263\n",
            "Loss: 0.046639375\n",
            "Loss: 0.046139717\n",
            "Loss: 0.04564812\n",
            "Loss: 0.045164432\n",
            "Loss: 0.04468851\n",
            "Loss: 0.04422019\n",
            "Loss: 0.043759324\n",
            "Loss: 0.04330577\n",
            "Loss: 0.042859383\n",
            "Loss: 0.04242003\n",
            "Loss: 0.041987587\n",
            "Loss: 0.04156191\n",
            "Loss: 0.041142873\n",
            "Loss: 0.04073036\n",
            "Loss: 0.04032425\n",
            "Loss: 0.0399244\n",
            "Loss: 0.039530724\n",
            "Loss: 0.0391431\n",
            "Loss: 0.0387614\n",
            "Loss: 0.038385537\n",
            "Loss: 0.03801539\n",
            "Loss: 0.037650876\n",
            "Loss: 0.03729187\n",
            "Loss: 0.03693828\n",
            "Loss: 0.036590014\n",
            "Loss: 0.03624697\n",
            "Loss: 0.035909068\n",
            "Loss: 0.035576195\n",
            "Loss: 0.035248283\n",
            "Loss: 0.034925234\n",
            "Loss: 0.034606963\n",
            "Loss: 0.034293387\n",
            "Loss: 0.03398443\n",
            "Loss: 0.03368\n",
            "Loss: 0.033380035\n",
            "Loss: 0.03308444\n",
            "Loss: 0.032793153\n",
            "Loss: 0.032506093\n",
            "Loss: 0.032223187\n",
            "Loss: 0.03194437\n",
            "Loss: 0.03166957\n",
            "Loss: 0.031398717\n",
            "Loss: 0.031131744\n",
            "Loss: 0.030868584\n",
            "Loss: 0.03060918\n",
            "Loss: 0.030353459\n",
            "Loss: 0.030101366\n",
            "Loss: 0.029852835\n",
            "Loss: 0.029607816\n",
            "Loss: 0.029366244\n",
            "Loss: 0.029128058\n",
            "Loss: 0.028893208\n",
            "Loss: 0.02866164\n",
            "Loss: 0.028433297\n",
            "Loss: 0.028208122\n",
            "Loss: 0.027986072\n",
            "Loss: 0.027767085\n",
            "Loss: 0.027551116\n",
            "Loss: 0.027338121\n",
            "Loss: 0.027128048\n",
            "Loss: 0.02692085\n",
            "Loss: 0.026716473\n",
            "Loss: 0.026514877\n",
            "Loss: 0.026316019\n",
            "Loss: 0.026119856\n",
            "Loss: 0.025926342\n",
            "Loss: 0.025735436\n",
            "Loss: 0.025547087\n",
            "Loss: 0.025361268\n",
            "Loss: 0.025177931\n",
            "Loss: 0.024997039\n",
            "Loss: 0.024818549\n",
            "Loss: 0.024642425\n",
            "Loss: 0.024468632\n",
            "Loss: 0.024297135\n",
            "Loss: 0.02412789\n",
            "Loss: 0.023960864\n",
            "Loss: 0.02379603\n",
            "Loss: 0.023633339\n",
            "Loss: 0.023472764\n",
            "Loss: 0.023314279\n",
            "Loss: 0.023157842\n",
            "Loss: 0.023003424\n",
            "Loss: 0.022850996\n",
            "Loss: 0.022700522\n",
            "Loss: 0.022551976\n",
            "Loss: 0.022405326\n",
            "Loss: 0.022260541\n",
            "Loss: 0.022117596\n",
            "Loss: 0.021976462\n",
            "Loss: 0.021837108\n",
            "Loss: 0.021699503\n",
            "Loss: 0.021563632\n",
            "Loss: 0.021429457\n",
            "Loss: 0.021296956\n",
            "Loss: 0.021166109\n",
            "Loss: 0.02103688\n",
            "Loss: 0.020909252\n",
            "Loss: 0.020783195\n",
            "Loss: 0.02065869\n",
            "Loss: 0.02053571\n",
            "Loss: 0.020414233\n",
            "Loss: 0.020294238\n",
            "Loss: 0.020175695\n",
            "Loss: 0.020058595\n",
            "Loss: 0.019942904\n",
            "Loss: 0.019828603\n",
            "Loss: 0.019715678\n",
            "Loss: 0.0196041\n",
            "Loss: 0.019493856\n",
            "Loss: 0.019384922\n",
            "Loss: 0.019277278\n",
            "Loss: 0.019170903\n",
            "Loss: 0.019065782\n",
            "Loss: 0.018961893\n",
            "Loss: 0.018859215\n",
            "Loss: 0.018757738\n",
            "Loss: 0.01865744\n",
            "Loss: 0.018558301\n",
            "Loss: 0.018460311\n",
            "Loss: 0.018363442\n",
            "Loss: 0.018267686\n",
            "Loss: 0.018173024\n",
            "Loss: 0.018079437\n",
            "Loss: 0.017986912\n",
            "Loss: 0.017895434\n",
            "Loss: 0.017804986\n",
            "Loss: 0.017715555\n",
            "Loss: 0.01762712\n",
            "Loss: 0.017539674\n",
            "Loss: 0.017453196\n",
            "Loss: 0.017367678\n",
            "Loss: 0.0172831\n",
            "Loss: 0.017199451\n",
            "Loss: 0.017116718\n",
            "Loss: 0.01703489\n",
            "Loss: 0.01695395\n",
            "Loss: 0.016873883\n",
            "Loss: 0.016794682\n",
            "Loss: 0.01671633\n",
            "Loss: 0.016638817\n",
            "Loss: 0.01656213\n",
            "Loss: 0.016486257\n",
            "Loss: 0.016411187\n",
            "Loss: 0.016336907\n",
            "Loss: 0.016263405\n",
            "Loss: 0.016190674\n",
            "Loss: 0.0161187\n",
            "Loss: 0.016047472\n",
            "Loss: 0.015976978\n",
            "Loss: 0.01590721\n",
            "Loss: 0.01583816\n",
            "Loss: 0.015769808\n",
            "Loss: 0.015702154\n",
            "Loss: 0.015635187\n",
            "Loss: 0.015568891\n",
            "Loss: 0.015503261\n",
            "Loss: 0.015438288\n",
            "Loss: 0.015373962\n",
            "Loss: 0.0153102735\n",
            "Loss: 0.015247215\n",
            "Loss: 0.015184772\n",
            "Loss: 0.015122942\n",
            "Loss: 0.015061715\n",
            "Loss: 0.015001083\n",
            "Loss: 0.014941034\n",
            "Loss: 0.014881563\n",
            "Loss: 0.014822661\n",
            "Loss: 0.014764321\n",
            "Loss: 0.014706534\n",
            "Loss: 0.014649293\n",
            "Loss: 0.014592591\n",
            "Loss: 0.014536419\n",
            "Loss: 0.014480771\n",
            "Loss: 0.014425636\n",
            "Loss: 0.014371013\n",
            "Loss: 0.014316892\n",
            "Loss: 0.014263269\n",
            "Loss: 0.01421013\n",
            "Loss: 0.014157474\n",
            "Loss: 0.014105293\n",
            "Loss: 0.01405358\n",
            "Loss: 0.014002331\n",
            "Loss: 0.013951535\n",
            "Loss: 0.01390119\n",
            "Loss: 0.013851291\n",
            "Loss: 0.013801827\n",
            "Loss: 0.013752796\n",
            "Loss: 0.013704189\n",
            "Loss: 0.0136560025\n",
            "Loss: 0.013608231\n",
            "Loss: 0.01356087\n",
            "Loss: 0.013513911\n",
            "Loss: 0.013467349\n",
            "Loss: 0.013421181\n",
            "Loss: 0.013375399\n",
            "Loss: 0.013330001\n",
            "Loss: 0.013284978\n",
            "Loss: 0.01324033\n",
            "Loss: 0.013196048\n",
            "Loss: 0.013152128\n",
            "Loss: 0.0131085655\n",
            "Loss: 0.01306536\n",
            "Loss: 0.013022498\n",
            "Loss: 0.0129799815\n",
            "Loss: 0.012937805\n",
            "Loss: 0.012895962\n",
            "Loss: 0.012854449\n",
            "Loss: 0.0128132645\n",
            "Loss: 0.012772399\n",
            "Loss: 0.012731854\n",
            "Loss: 0.012691621\n",
            "Loss: 0.012651699\n",
            "Loss: 0.01261208\n",
            "Loss: 0.012572768\n",
            "Loss: 0.01253375\n",
            "Loss: 0.012495028\n",
            "Loss: 0.012456593\n",
            "Loss: 0.012418445\n",
            "Loss: 0.012380583\n",
            "Loss: 0.012343002\n",
            "Loss: 0.012305693\n",
            "Loss: 0.012268658\n",
            "Loss: 0.012231892\n",
            "Loss: 0.012195388\n",
            "Loss: 0.012159151\n",
            "Loss: 0.012123173\n",
            "Loss: 0.012087448\n",
            "Loss: 0.012051977\n",
            "Loss: 0.012016756\n",
            "Loss: 0.011981782\n",
            "Loss: 0.011947049\n",
            "Loss: 0.011912558\n",
            "Loss: 0.011878303\n",
            "Loss: 0.011844285\n",
            "Loss: 0.0118104955\n",
            "Loss: 0.011776937\n",
            "Loss: 0.011743605\n",
            "Loss: 0.011710494\n",
            "Loss: 0.011677607\n",
            "Loss: 0.011644935\n",
            "Loss: 0.011612479\n",
            "Loss: 0.011580236\n",
            "Loss: 0.011548204\n",
            "Loss: 0.011516379\n",
            "Loss: 0.011484759\n",
            "Loss: 0.011453343\n",
            "Loss: 0.011422125\n",
            "Loss: 0.011391106\n",
            "Loss: 0.011360284\n",
            "Loss: 0.011329656\n",
            "Loss: 0.011299217\n",
            "Loss: 0.011268968\n",
            "Loss: 0.011238906\n",
            "Loss: 0.011209027\n",
            "Loss: 0.011179332\n",
            "Loss: 0.011149816\n",
            "Loss: 0.011120478\n",
            "Loss: 0.011091317\n",
            "Loss: 0.011062332\n",
            "Loss: 0.011033518\n",
            "Loss: 0.011004874\n",
            "Loss: 0.010976398\n",
            "Loss: 0.010948088\n",
            "Loss: 0.0109199425\n",
            "Loss: 0.010891961\n",
            "Loss: 0.010864141\n",
            "Loss: 0.010836481\n",
            "Loss: 0.0108089745\n",
            "Loss: 0.010781627\n",
            "Loss: 0.010754431\n",
            "Loss: 0.010727388\n",
            "Loss: 0.010700495\n",
            "Loss: 0.01067375\n",
            "Loss: 0.010647156\n",
            "Loss: 0.010620705\n",
            "Loss: 0.010594399\n",
            "Loss: 0.010568235\n",
            "Loss: 0.010542213\n",
            "Loss: 0.010516329\n",
            "Loss: 0.010490582\n",
            "Loss: 0.010464974\n",
            "Loss: 0.010439499\n",
            "Loss: 0.010414161\n",
            "Loss: 0.010388954\n",
            "Loss: 0.010363877\n",
            "Loss: 0.010338929\n",
            "Loss: 0.010314111\n",
            "Loss: 0.010289418\n",
            "Loss: 0.010264851\n",
            "Loss: 0.0102404095\n",
            "Loss: 0.01021609\n",
            "Loss: 0.010191891\n",
            "Loss: 0.010167813\n",
            "Loss: 0.010143856\n",
            "Loss: 0.010120017\n",
            "Loss: 0.010096294\n",
            "Loss: 0.010072687\n",
            "Loss: 0.010049194\n",
            "Loss: 0.010025813\n",
            "Loss: 0.010002546\n",
            "Loss: 0.00997939\n",
            "Loss: 0.009956344\n",
            "Loss: 0.009933407\n",
            "Loss: 0.009910579\n",
            "Loss: 0.009887857\n",
            "Loss: 0.00986524\n",
            "Loss: 0.009842729\n",
            "Loss: 0.009820323\n",
            "Loss: 0.009798016\n",
            "Loss: 0.009775813\n",
            "Loss: 0.009753713\n",
            "Loss: 0.00973171\n",
            "Loss: 0.009709808\n",
            "Loss: 0.009688002\n",
            "Loss: 0.009666295\n",
            "Loss: 0.009644683\n",
            "Loss: 0.009623167\n",
            "Loss: 0.009601746\n",
            "Loss: 0.009580417\n",
            "Loss: 0.009559181\n",
            "Loss: 0.009538037\n",
            "Loss: 0.009516985\n",
            "Loss: 0.009496022\n",
            "Loss: 0.009475151\n",
            "Loss: 0.009454367\n",
            "Loss: 0.009433671\n",
            "Loss: 0.009413062\n",
            "Loss: 0.009392538\n",
            "Loss: 0.009372103\n",
            "Loss: 0.009351751\n",
            "Loss: 0.009331482\n",
            "Loss: 0.009311299\n",
            "Loss: 0.009291197\n",
            "Loss: 0.0092711765\n",
            "Loss: 0.009251238\n",
            "Loss: 0.00923138\n",
            "Loss: 0.009211601\n",
            "Loss: 0.009191902\n",
            "Loss: 0.009172281\n",
            "Loss: 0.009152737\n",
            "Loss: 0.009133272\n",
            "Loss: 0.009113884\n",
            "Loss: 0.009094571\n",
            "Loss: 0.009075333\n",
            "Loss: 0.0090561705\n",
            "Loss: 0.009037081\n",
            "Loss: 0.009018066\n",
            "Loss: 0.008999125\n",
            "Loss: 0.008980255\n",
            "Loss: 0.008961455\n",
            "Loss: 0.00894273\n",
            "Loss: 0.008924073\n",
            "Loss: 0.008905487\n",
            "Loss: 0.008886971\n",
            "Loss: 0.008868522\n",
            "Loss: 0.008850142\n",
            "Loss: 0.008831832\n",
            "Loss: 0.008813588\n",
            "Loss: 0.008795409\n",
            "Loss: 0.008777299\n",
            "Loss: 0.008759256\n",
            "Loss: 0.0087412745\n",
            "Loss: 0.0087233605\n",
            "Loss: 0.008705511\n",
            "Loss: 0.008687724\n",
            "Loss: 0.008670004\n",
            "Loss: 0.0086523425\n",
            "Loss: 0.008634746\n",
            "Loss: 0.00861721\n",
            "Loss: 0.008599738\n",
            "Loss: 0.008582325\n",
            "Loss: 0.008564975\n",
            "Loss: 0.008547682\n",
            "Loss: 0.00853045\n",
            "Loss: 0.008513278\n",
            "Loss: 0.008496168\n",
            "Loss: 0.008479114\n",
            "Loss: 0.008462117\n",
            "Loss: 0.00844518\n",
            "Loss: 0.0084283\n",
            "Loss: 0.008411476\n",
            "Loss: 0.008394709\n",
            "Loss: 0.008378\n",
            "Loss: 0.008361343\n",
            "Loss: 0.008344747\n",
            "Loss: 0.008328202\n",
            "Loss: 0.008311713\n",
            "Loss: 0.008295278\n",
            "Loss: 0.008278897\n",
            "Loss: 0.008262571\n",
            "Loss: 0.008246297\n",
            "Loss: 0.008230076\n",
            "Loss: 0.008213908\n",
            "Loss: 0.008197793\n",
            "Loss: 0.00818173\n",
            "Loss: 0.008165718\n",
            "Loss: 0.008149757\n",
            "Loss: 0.008133847\n",
            "Loss: 0.00811799\n",
            "Loss: 0.008102182\n",
            "Loss: 0.008086424\n",
            "Loss: 0.008070715\n",
            "Loss: 0.0080550555\n",
            "Loss: 0.008039447\n",
            "Loss: 0.008023884\n",
            "Loss: 0.008008373\n",
            "Loss: 0.007992908\n",
            "Loss: 0.007977493\n",
            "Loss: 0.007962125\n",
            "Loss: 0.007946803\n",
            "Loss: 0.0079315305\n",
            "Loss: 0.007916303\n",
            "Loss: 0.007901122\n",
            "Loss: 0.007885989\n",
            "Loss: 0.007870901\n",
            "Loss: 0.007855859\n",
            "Loss: 0.007840863\n",
            "Loss: 0.00782591\n",
            "Loss: 0.0078110043\n",
            "Loss: 0.007796144\n",
            "Loss: 0.0077813272\n",
            "Loss: 0.007766553\n",
            "Loss: 0.0077518253\n",
            "Loss: 0.0077371392\n",
            "Loss: 0.0077224993\n",
            "Loss: 0.0077079004\n",
            "Loss: 0.0076933457\n",
            "Loss: 0.007678833\n",
            "Loss: 0.007664365\n",
            "Loss: 0.0076499376\n",
            "Loss: 0.007635553\n",
            "Loss: 0.007621209\n",
            "Loss: 0.007606908\n",
            "Loss: 0.0075926487\n",
            "Loss: 0.00757843\n",
            "Loss: 0.0075642527\n",
            "Loss: 0.0075501166\n",
            "Loss: 0.0075360197\n",
            "Loss: 0.0075219637\n",
            "Loss: 0.0075079487\n",
            "Loss: 0.0074939732\n",
            "Loss: 0.0074800365\n",
            "Loss: 0.0074661416\n",
            "Loss: 0.0074522854\n",
            "Loss: 0.0074384687\n",
            "Loss: 0.0074246894\n",
            "Loss: 0.0074109496\n",
            "Loss: 0.0073972493\n",
            "Loss: 0.0073835873\n",
            "Loss: 0.007369963\n",
            "Loss: 0.007356377\n",
            "Loss: 0.007342829\n",
            "Loss: 0.0073293187\n",
            "Loss: 0.0073158466\n",
            "Loss: 0.00730241\n",
            "Loss: 0.0072890134\n",
            "Loss: 0.007275651\n",
            "Loss: 0.0072623272\n",
            "Loss: 0.0072490387\n",
            "Loss: 0.0072357883\n",
            "Loss: 0.0072225733\n",
            "Loss: 0.007209394\n",
            "Loss: 0.0071962513\n",
            "Loss: 0.007183144\n",
            "Loss: 0.007170073\n",
            "Loss: 0.0071570366\n",
            "Loss: 0.0071440376\n",
            "Loss: 0.0071310704\n",
            "Loss: 0.0071181413\n",
            "Loss: 0.0071052443\n",
            "Loss: 0.007092384\n",
            "Loss: 0.007079557\n",
            "Loss: 0.007066766\n",
            "Loss: 0.007054009\n",
            "Loss: 0.0070412857\n",
            "Loss: 0.007028595\n",
            "Loss: 0.00701594\n",
            "Loss: 0.0070033167\n",
            "Loss: 0.0069907294\n",
            "Loss: 0.0069781733\n",
            "Loss: 0.006965652\n",
            "Loss: 0.0069531645\n",
            "Loss: 0.0069407076\n",
            "Loss: 0.0069282847\n",
            "Loss: 0.0069158953\n",
            "Loss: 0.006903537\n",
            "Loss: 0.0068912115\n",
            "Loss: 0.006878919\n",
            "Loss: 0.0068666567\n",
            "Loss: 0.0068544294\n",
            "Loss: 0.006842232\n",
            "Loss: 0.0068300664\n",
            "Loss: 0.0068179327\n",
            "Loss: 0.0068058306\n",
            "Loss: 0.0067937607\n",
            "Loss: 0.00678172\n",
            "Loss: 0.0067697125\n",
            "Loss: 0.006757735\n",
            "Loss: 0.006745789\n",
            "Loss: 0.0067338725\n",
            "Loss: 0.006721988\n",
            "Loss: 0.006710133\n",
            "Loss: 0.0066983085\n",
            "Loss: 0.006686515\n",
            "Loss: 0.006674752\n",
            "Loss: 0.006663018\n",
            "Loss: 0.0066513144\n",
            "Loss: 0.00663964\n",
            "Loss: 0.006627996\n",
            "Loss: 0.0066163815\n",
            "Loss: 0.006604795\n",
            "Loss: 0.0065932404\n",
            "Loss: 0.0065817134\n",
            "Loss: 0.006570216\n",
            "Loss: 0.0065587475\n",
            "Loss: 0.006547308\n",
            "Loss: 0.006535897\n",
            "Loss: 0.006524516\n",
            "Loss: 0.006513163\n",
            "Loss: 0.0065018386\n",
            "Loss: 0.006490541\n",
            "Loss: 0.006479274\n",
            "Loss: 0.006468034\n",
            "Loss: 0.006456822\n",
            "Loss: 0.0064456393\n",
            "Loss: 0.006434483\n",
            "Loss: 0.0064233555\n",
            "Loss: 0.0064122546\n",
            "Loss: 0.0064011817\n",
            "Loss: 0.0063901357\n",
            "Loss: 0.0063791187\n",
            "Loss: 0.006368128\n",
            "Loss: 0.006357165\n",
            "Loss: 0.0063462295\n",
            "Loss: 0.0063353195\n",
            "Loss: 0.0063244374\n",
            "Loss: 0.0063135815\n",
            "Loss: 0.0063027544\n",
            "Loss: 0.0062919515\n",
            "Loss: 0.006281176\n",
            "Loss: 0.0062704277\n",
            "Loss: 0.0062597054\n",
            "Loss: 0.0062490087\n",
            "Loss: 0.0062383385\n",
            "Loss: 0.006227694\n",
            "Loss: 0.006217078\n",
            "Loss: 0.006206485\n",
            "Loss: 0.006195919\n",
            "Loss: 0.0061853784\n",
            "Loss: 0.0061748642\n",
            "Loss: 0.0061643748\n",
            "Loss: 0.0061539127\n",
            "Loss: 0.0061434726\n",
            "Loss: 0.0061330604\n",
            "Loss: 0.0061226725\n",
            "Loss: 0.00611231\n",
            "Loss: 0.006101973\n",
            "Loss: 0.0060916604\n",
            "Loss: 0.006081373\n",
            "Loss: 0.006071111\n",
            "Loss: 0.006060873\n",
            "Loss: 0.0060506607\n",
            "Loss: 0.0060404716\n",
            "Loss: 0.0060303076\n",
            "Loss: 0.0060201692\n",
            "Loss: 0.0060100537\n",
            "Loss: 0.005999962\n",
            "Loss: 0.0059898947\n",
            "Loss: 0.0059798528\n",
            "Loss: 0.0059698354\n",
            "Loss: 0.005959841\n",
            "Loss: 0.005949871\n",
            "Loss: 0.0059399237\n",
            "Loss: 0.005930001\n",
            "Loss: 0.005920103\n",
            "Loss: 0.0059102266\n",
            "Loss: 0.0059003737\n",
            "Loss: 0.0058905454\n",
            "Loss: 0.0058807395\n",
            "Loss: 0.005870959\n",
            "Loss: 0.0058612\n",
            "Loss: 0.005851464\n",
            "Loss: 0.0058417525\n",
            "Loss: 0.0058320635\n",
            "Loss: 0.0058223973\n",
            "Loss: 0.005812754\n",
            "Loss: 0.0058031334\n",
            "Loss: 0.0057935356\n",
            "Loss: 0.00578396\n",
            "Loss: 0.005774408\n",
            "Loss: 0.0057648774\n",
            "Loss: 0.0057553696\n",
            "Loss: 0.0057458864\n",
            "Loss: 0.005736423\n",
            "Loss: 0.0057269824\n",
            "Loss: 0.0057175644\n",
            "Loss: 0.0057081673\n",
            "Loss: 0.005698794\n",
            "Loss: 0.005689442\n",
            "Loss: 0.0056801126\n",
            "Loss: 0.0056708027\n",
            "Loss: 0.005661519\n",
            "Loss: 0.0056522544\n",
            "Loss: 0.0056430115\n",
            "Loss: 0.00563379\n",
            "Loss: 0.00562459\n",
            "Loss: 0.005615412\n",
            "Loss: 0.0056062555\n",
            "Loss: 0.00559712\n",
            "Loss: 0.0055880076\n",
            "Loss: 0.0055789137\n",
            "Loss: 0.0055698426\n",
            "Loss: 0.0055607916\n",
            "Loss: 0.0055517624\n",
            "Loss: 0.0055427547\n",
            "Loss: 0.005533767\n",
            "Loss: 0.0055248016\n",
            "Loss: 0.0055158557\n",
            "Loss: 0.005506931\n",
            "Loss: 0.005498026\n",
            "Loss: 0.0054891435\n",
            "Loss: 0.005480281\n",
            "Loss: 0.0054714386\n",
            "Loss: 0.005462617\n",
            "Loss: 0.005453816\n",
            "Loss: 0.0054450347\n",
            "Loss: 0.0054362733\n",
            "Loss: 0.0054275333\n",
            "Loss: 0.0054188133\n",
            "Loss: 0.005410113\n",
            "Loss: 0.005401434\n",
            "Loss: 0.0053927735\n",
            "Loss: 0.0053841337\n",
            "Loss: 0.005375514\n",
            "Loss: 0.0053669135\n",
            "Loss: 0.0053583323\n",
            "Loss: 0.005349772\n",
            "Loss: 0.0053412323\n",
            "Loss: 0.0053327107\n",
            "Loss: 0.005324208\n",
            "Loss: 0.0053157266\n",
            "Loss: 0.005307263\n",
            "Loss: 0.0052988203\n",
            "Loss: 0.0052903974\n",
            "Loss: 0.0052819927\n",
            "Loss: 0.0052736066\n",
            "Loss: 0.0052652406\n",
            "Loss: 0.005256894\n",
            "Loss: 0.0052485657\n",
            "Loss: 0.005240257\n",
            "Loss: 0.0052319677\n",
            "Loss: 0.005223696\n",
            "Loss: 0.0052154446\n",
            "Loss: 0.005207212\n",
            "Loss: 0.005198998\n",
            "Loss: 0.0051908023\n",
            "Loss: 0.0051826257\n",
            "Loss: 0.0051744673\n",
            "Loss: 0.0051663285\n",
            "Loss: 0.0051582064\n",
            "Loss: 0.005150105\n",
            "Loss: 0.0051420205\n",
            "Loss: 0.005133955\n",
            "Loss: 0.0051259077\n",
            "Loss: 0.0051178783\n",
            "Loss: 0.005109868\n",
            "Loss: 0.005101875\n",
            "Loss: 0.005093901\n",
            "Loss: 0.005085945\n",
            "Loss: 0.0050780075\n",
            "Loss: 0.0050700866\n",
            "Loss: 0.0050621843\n",
            "Loss: 0.005054301\n",
            "Loss: 0.005046434\n",
            "Loss: 0.0050385855\n",
            "Loss: 0.005030755\n",
            "Loss: 0.0050229416\n",
            "Loss: 0.005015147\n",
            "Loss: 0.0050073685\n",
            "Loss: 0.004999608\n",
            "Loss: 0.0049918653\n",
            "Loss: 0.0049841404\n",
            "Loss: 0.0049764323\n",
            "Loss: 0.004968742\n",
            "Loss: 0.0049610687\n",
            "Loss: 0.0049534137\n",
            "Loss: 0.0049457755\n",
            "Loss: 0.004938155\n",
            "Loss: 0.00493055\n",
            "Loss: 0.0049229646\n",
            "Loss: 0.0049153944\n",
            "Loss: 0.004907842\n",
            "Loss: 0.004900306\n",
            "Loss: 0.004892787\n",
            "Loss: 0.0048852856\n",
            "Loss: 0.004877801\n",
            "Loss: 0.0048703323\n",
            "Loss: 0.004862881\n",
            "Loss: 0.0048554474\n",
            "Loss: 0.0048480295\n",
            "Loss: 0.004840628\n",
            "Loss: 0.004833244\n",
            "Loss: 0.0048258766\n",
            "Loss: 0.0048185242\n",
            "Loss: 0.00481119\n",
            "Loss: 0.004803871\n",
            "Loss: 0.0047965683\n",
            "Loss: 0.0047892835\n",
            "Loss: 0.0047820136\n",
            "Loss: 0.0047747614\n",
            "Loss: 0.0047675236\n",
            "Loss: 0.004760303\n",
            "Loss: 0.004753098\n",
            "Loss: 0.00474591\n",
            "Loss: 0.0047387374\n",
            "Loss: 0.004731582\n",
            "Loss: 0.0047244406\n",
            "Loss: 0.0047173174\n",
            "Loss: 0.0047102077\n",
            "Loss: 0.004703116\n",
            "Loss: 0.0046960386\n",
            "Loss: 0.004688977\n",
            "Loss: 0.0046819313\n",
            "Loss: 0.004674902\n",
            "Loss: 0.0046678893\n",
            "Loss: 0.00466089\n",
            "Loss: 0.0046539083\n",
            "Loss: 0.00464694\n",
            "Loss: 0.004639989\n",
            "Loss: 0.0046330527\n",
            "Loss: 0.004626132\n",
            "Loss: 0.0046192273\n",
            "Loss: 0.004612338\n",
            "Loss: 0.0046054632\n",
            "Loss: 0.004598603\n",
            "Loss: 0.0045917593\n",
            "Loss: 0.004584931\n",
            "Loss: 0.0045781173\n",
            "Loss: 0.0045713186\n",
            "Loss: 0.0045645353\n",
            "Loss: 0.004557767\n",
            "Loss: 0.004551014\n",
            "Loss: 0.004544275\n",
            "Loss: 0.004537552\n",
            "Loss: 0.0045308433\n",
            "Loss: 0.0045241495\n",
            "Loss: 0.0045174705\n",
            "Loss: 0.0045108064\n",
            "Loss: 0.004504157\n",
            "Loss: 0.004497523\n",
            "Loss: 0.004490903\n",
            "Loss: 0.0044842977\n",
            "Loss: 0.004477707\n",
            "Loss: 0.004471131\n",
            "Loss: 0.0044645695\n",
            "Loss: 0.0044580237\n",
            "Loss: 0.0044514905\n",
            "Loss: 0.0044449726\n",
            "Loss: 0.0044384687\n",
            "Loss: 0.0044319797\n",
            "Loss: 0.0044255047\n",
            "Loss: 0.0044190427\n",
            "Loss: 0.004412596\n",
            "Loss: 0.004406166\n",
            "Loss: 0.004399747\n",
            "Loss: 0.004393343\n",
            "Loss: 0.004386954\n",
            "Loss: 0.0043805786\n",
            "Loss: 0.0043742163\n",
            "Loss: 0.0043678684\n",
            "Loss: 0.0043615354\n",
            "Loss: 0.004355216\n",
            "Loss: 0.0043489104\n",
            "Loss: 0.0043426184\n",
            "Loss: 0.0043363404\n",
            "Loss: 0.0043300767\n",
            "Loss: 0.0043238257\n",
            "Loss: 0.0043175896\n",
            "Loss: 0.0043113665\n",
            "Loss: 0.004305157\n",
            "Loss: 0.0042989617\n",
            "Loss: 0.00429278\n",
            "Loss: 0.004286611\n",
            "Loss: 0.004280457\n",
            "Loss: 0.004274315\n",
            "Loss: 0.004268187\n",
            "Loss: 0.0042620734\n",
            "Loss: 0.0042559723\n",
            "Loss: 0.0042498847\n",
            "Loss: 0.00424381\n",
            "Loss: 0.00423775\n",
            "Loss: 0.0042317025\n",
            "Loss: 0.0042256676\n",
            "Loss: 0.0042196466\n",
            "Loss: 0.004213639\n",
            "Loss: 0.004207644\n",
            "Loss: 0.004201663\n",
            "Loss: 0.004195694\n",
            "Loss: 0.004189739\n",
            "Loss: 0.0041837962\n",
            "Loss: 0.004177867\n",
            "Loss: 0.0041719507\n",
            "Loss: 0.004166047\n",
            "Loss: 0.004160157\n",
            "Loss: 0.004154279\n",
            "Loss: 0.0041484144\n",
            "Loss: 0.004142562\n",
            "Loss: 0.0041367235\n",
            "Loss: 0.004130896\n",
            "Loss: 0.0041250833\n",
            "Loss: 0.004119282\n",
            "Loss: 0.0041134935\n",
            "Loss: 0.004107718\n",
            "Loss: 0.004101955\n",
            "Loss: 0.0040962044\n",
            "Loss: 0.0040904665\n",
            "Loss: 0.0040847417\n",
            "Loss: 0.004079029\n",
            "Loss: 0.0040733283\n",
            "Loss: 0.0040676403\n",
            "Loss: 0.004061964\n",
            "Loss: 0.004056302\n",
            "Loss: 0.0040506506\n",
            "Loss: 0.004045012\n",
            "Loss: 0.004039386\n",
            "Loss: 0.0040337713\n",
            "Loss: 0.00402817\n",
            "Loss: 0.00402258\n",
            "Loss: 0.0040170024\n",
            "Loss: 0.004011437\n",
            "Loss: 0.004005884\n",
            "Loss: 0.004000343\n",
            "Loss: 0.0039948137\n",
            "Loss: 0.003989297\n",
            "Loss: 0.003983792\n",
            "Loss: 0.0039782985\n",
            "Loss: 0.003972817\n",
            "Loss: 0.0039673494\n",
            "Loss: 0.0039618914\n",
            "Loss: 0.003956445\n",
            "Loss: 0.0039510117\n",
            "Loss: 0.0039455895\n",
            "Loss: 0.00394018\n",
            "Loss: 0.003934782\n",
            "Loss: 0.003929395\n",
            "Loss: 0.0039240206\n",
            "Loss: 0.003918657\n",
            "Loss: 0.0039133052\n",
            "Loss: 0.0039079655\n",
            "Loss: 0.0039026362\n",
            "Loss: 0.0038973198\n",
            "Loss: 0.0038920154\n",
            "Loss: 0.0038867209\n",
            "Loss: 0.0038814393\n",
            "Loss: 0.003876168\n",
            "Loss: 0.0038709084\n",
            "Loss: 0.0038656606\n",
            "Loss: 0.0038604243\n",
            "Loss: 0.0038551984\n",
            "Loss: 0.003849984\n",
            "Loss: 0.0038447825\n",
            "Loss: 0.0038395906\n",
            "Loss: 0.0038344106\n",
            "Loss: 0.0038292406\n",
            "Loss: 0.0038240822\n",
            "Loss: 0.003818936\n",
            "Loss: 0.0038138004\n",
            "Loss: 0.003808676\n",
            "Loss: 0.0038035626\n",
            "Loss: 0.0037984604\n",
            "Loss: 0.003793369\n",
            "Loss: 0.0037882884\n",
            "Loss: 0.0037832188\n",
            "Loss: 0.0037781615\n",
            "Loss: 0.0037731135\n",
            "Loss: 0.003768077\n",
            "Loss: 0.0037630517\n",
            "Loss: 0.0037580365\n",
            "Loss: 0.003753033\n",
            "Loss: 0.0037480392\n",
            "Loss: 0.0037430567\n",
            "Loss: 0.0037380853\n",
            "Loss: 0.003733124\n",
            "Loss: 0.003728174\n",
            "Loss: 0.003723234\n",
            "Loss: 0.003718306\n",
            "Loss: 0.0037133866\n",
            "Loss: 0.003708479\n",
            "Loss: 0.0037035816\n",
            "Loss: 0.003698696\n",
            "Loss: 0.00369382\n",
            "Loss: 0.003688954\n",
            "Loss: 0.0036840995\n",
            "Loss: 0.003679255\n",
            "Loss: 0.00367442\n",
            "Loss: 0.003669596\n",
            "Loss: 0.0036647832\n",
            "Loss: 0.0036599806\n",
            "Loss: 0.003655188\n",
            "Loss: 0.0036504064\n",
            "Loss: 0.0036456333\n",
            "Loss: 0.0036408724\n",
            "Loss: 0.0036361206\n",
            "Loss: 0.0036313788\n",
            "Loss: 0.0036266479\n",
            "Predictions: [[0.81670046]\n",
            " [0.9681659 ]\n",
            " [1.0400479 ]\n",
            " [0.72243035]\n",
            " [0.7232375 ]\n",
            " [0.7513404 ]\n",
            " [0.3678133 ]\n",
            " [0.91154563]\n",
            " [0.24747363]\n",
            " [0.69941556]]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "# Generate some sample data\n",
        "data = np.random.rand(100, 2)\n",
        "labels = np.sin(data[:, 0] + data[:, 1])\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "data = scaler.fit_transform(data)\n",
        "\n",
        "# Use K-Means clustering to find RBF centers\n",
        "kmeans = KMeans(n_clusters=10, random_state=0).fit(data)\n",
        "centers = kmeans.cluster_centers_\n",
        "\n",
        "# Create a TensorFlow graph\n",
        "input_placeholder = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, 2])\n",
        "centers_placeholder = tf.compat.v1.placeholder(dtype=tf.float32, shape=[10, 2])\n",
        "betas_placeholder = tf.compat.v1.placeholder(dtype=tf.float32, shape=[10])\n",
        "\n",
        "# Calculate the distance between data points and RBF centers\n",
        "distances = tf.norm(input_placeholder[:, tf.newaxis] - centers_placeholder, axis=2, ord=2)\n",
        "\n",
        "# Define the RBF layer\n",
        "rbf_layer = tf.exp(-betas_placeholder * distances)\n",
        "\n",
        "# Create the output layer (fully connected)\n",
        "weights = tf.compat.v1.Variable(tf.compat.v1.random_normal([10, 1]))\n",
        "biases = tf.compat.v1.Variable(tf.compat.v1.random_normal([1]))\n",
        "output = tf.compat.v1.matmul(rbf_layer, weights) + biases\n",
        "\n",
        "# Define the target values\n",
        "target = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, 1])\n",
        "\n",
        "# Define the loss function (MSE)\n",
        "loss = tf.reduce_mean(tf.square(target - output))\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.1)\n",
        "train_op = optimizer.minimize(loss)\n",
        "\n",
        "# Create a TensorFlow session and train the RBFN\n",
        "with tf.compat.v1.Session() as sess:\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "    for _ in range(1000):\n",
        "        feed_dict = {\n",
        "            input_placeholder: data,\n",
        "            centers_placeholder: centers,\n",
        "            betas_placeholder: np.ones(10),\n",
        "            target: labels.reshape(-1, 1)\n",
        "        }\n",
        "        _, current_loss = sess.run([train_op, loss], feed_dict=feed_dict)\n",
        "        print(\"Loss:\", current_loss)\n",
        "\n",
        "    # Evaluate the RBFN\n",
        "    test_data = np.random.rand(10, 2)\n",
        "    test_data = scaler.transform(test_data)\n",
        "    feed_dict = {\n",
        "        input_placeholder: test_data,\n",
        "        centers_placeholder: centers,\n",
        "        betas_placeholder: np.ones(10)\n",
        "    }\n",
        "    predictions = sess.run(output, feed_dict=feed_dict)\n",
        "\n",
        "    print(\"Predictions:\", predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6AKpE2zLmIJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "data",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}